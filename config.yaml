hf:
    token: ................................
    pretrained_model_id: "mistralai/Mistral-7B-Instruct-v0.3"
    target_model_id: "SleMary/mistral-7b-v0.3-instruct-norobots"
    quantized_model_id: "SleMary/mistral-7b-v0.3-instruct-norobots-8bit-guff"

wandb:
    token: .................................

lora_config:
    r: 32
    lora_alpha: 64
    target_modules:
        - "q_proj"
        - "k_proj"
        - "v_proj"
        - "o_proj"
        - "gate_proj"
        - "up_proj"
        - "down_proj"
        - "lm_head"
    bias: "none"
    lora_dropout: 0.05
    task_type: "CAUSAL_LM"

train_params:
    output_dir: "Mistral-7B-Instruct-v0.3-norobots"
    max_seq_length: 768
    num_train_epochs: 1
    per_device_train_batch_size: 8
    learning_rate: 1e-5
    optim: "sgd"
